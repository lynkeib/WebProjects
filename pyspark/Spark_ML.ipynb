{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Start Spark Session\n",
    "from pyspark.sql.types import * # Type changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master('local[*]')\\\n",
    ".appName('ML')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.25.158.238:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11b1c8ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/CogsleyServices-SalaData-US.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.load(path,\n",
    "                      format = 'csv',\n",
    "                      header = 'true',\n",
    "                      inferSchema = 'true').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(RowID=1914, OrderID=13729, OrderDate=datetime.datetime(2009, 1, 1, 0, 0), OrderMonthYear=datetime.datetime(2009, 1, 1, 0, 0), Quantity=9, Quote=1800, DiscountPct=0.08, Rate=200, SaleAmount=1640.96, CustomerName=u'Matt Bertelsons', CompanyName=u'The Priceline Group Inc.', Sector=u'Miscellaneous', Industry=u'Business Services', City=u'Bowie', ZipCode=20715, State=u'Maryland', Region=u'East', ProjectCompleteDate=datetime.datetime(2009, 1, 3, 0, 0), DaystoComplete=2, ProductKey=u'Development - Big Data', ProductCategory=u'Development', ProductSubCategory=u'Python', Consultant=u'Noah Smith', Manager=u'Allen Young', HourlyWage=59, RowCount=1, WageMargin\\=u'0.71\\\\'),\n",
       " Row(RowID=4031, OrderID=28774, OrderDate=datetime.datetime(2009, 1, 1, 0, 0), OrderMonthYear=datetime.datetime(2009, 1, 1, 0, 0), Quantity=32, Quote=6400, DiscountPct=0.1, Rate=200, SaleAmount=5707.67, CustomerName=u'Jessica Thornton', CompanyName=u'Garmin Ltd.', Sector=u'Capital Goods', Industry=u'Industrial Machinery/Components', City=u'McKeesport', ZipCode=15131, State=u'Pennsylvania', Region=u'East', ProjectCompleteDate=datetime.datetime(2009, 1, 2, 0, 0), DaystoComplete=1, ProductKey=u'Development - Big Data', ProductCategory=u'Development', ProductSubCategory=u'Market Research', Consultant=u'Daniel Tusk', Manager=u'Allen Young', HourlyWage=45, RowCount=1, WageMargin\\=u'0.78\\\\')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  = data.dropna()\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RowID: integer (nullable = true)\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- OrderDate: timestamp (nullable = true)\n",
      " |-- OrderMonthYear: timestamp (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Quote: integer (nullable = true)\n",
      " |-- DiscountPct: double (nullable = true)\n",
      " |-- Rate: integer (nullable = true)\n",
      " |-- SaleAmount: double (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- Sector: string (nullable = true)\n",
      " |-- Industry: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZipCode: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- ProjectCompleteDate: timestamp (nullable = true)\n",
      " |-- DaystoComplete: integer (nullable = true)\n",
      " |-- ProductKey: string (nullable = true)\n",
      " |-- ProductCategory: string (nullable = true)\n",
      " |-- ProductSubCategory: string (nullable = true)\n",
      " |-- Consultant: string (nullable = true)\n",
      " |-- Manager: string (nullable = true)\n",
      " |-- HourlyWage: integer (nullable = true)\n",
      " |-- RowCount: integer (nullable = true)\n",
      " |-- WageMargin\\: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data\\\n",
    ".select(data[\"OrderMonthYear\"], data[\"SaleAmount\"])\\\n",
    ".groupby(\"OrderMonthYear\")\\\n",
    ".sum()\\\n",
    ".orderBy(\"OrderMonthYear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|     OrderMonthYear|  sum(SaleAmount)|\n",
      "+-------------------+-----------------+\n",
      "|2009-01-01 00:00:00|734559.3599999996|\n",
      "|2009-02-01 00:00:00|539887.7999999998|\n",
      "|2009-03-01 00:00:00|559449.9600000002|\n",
      "|2009-04-01 00:00:00|        614983.31|\n",
      "|2009-05-01 00:00:00|637481.3899999998|\n",
      "|2009-06-01 00:00:00|555516.2199999999|\n",
      "|2009-07-01 00:00:00|        670807.13|\n",
      "|2009-08-01 00:00:00|660353.6399999995|\n",
      "|2009-09-01 00:00:00|        648992.75|\n",
      "|2009-10-01 00:00:00|570492.5099999999|\n",
      "|2009-11-01 00:00:00|566171.2399999998|\n",
      "|2009-12-01 00:00:00|560232.9200000002|\n",
      "|2010-01-01 00:00:00|577908.3700000001|\n",
      "|2010-02-01 00:00:00|586651.7600000002|\n",
      "|2010-03-01 00:00:00|529839.6000000002|\n",
      "|2010-04-01 00:00:00|490967.0900000001|\n",
      "|2010-05-01 00:00:00|748098.0000000003|\n",
      "|2010-06-01 00:00:00|611528.2600000001|\n",
      "|2010-07-01 00:00:00|584327.2599999997|\n",
      "|2010-08-01 00:00:00|622679.6900000004|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary.withColumn(\"OrderMonthYear\", summary['OrderMonthYear'].substr(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summary\\\n",
    ".rdd\\\n",
    ".map(lambda r: (int(r['OrderMonthYear'].replace('-','')), r['sum(SaleAmount)']))\\\n",
    ".toDF()\\\n",
    ".selectExpr(\"_1 as OrderMonthYear\",\"_2 as sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|OrderMonthYear|              sum|\n",
      "+--------------+-----------------+\n",
      "|      20090101|734559.3599999996|\n",
      "|      20090201|539887.7999999998|\n",
      "|      20090301|559449.9600000002|\n",
      "|      20090401|        614983.31|\n",
      "|      20090501|637481.3899999998|\n",
      "|      20090601|555516.2199999999|\n",
      "|      20090701|        670807.13|\n",
      "|      20090801|660353.6399999995|\n",
      "|      20090901|        648992.75|\n",
      "|      20091001|570492.5099999999|\n",
      "|      20091101|566171.2399999998|\n",
      "|      20091201|560232.9200000002|\n",
      "|      20100101|577908.3700000001|\n",
      "|      20100201|586651.7600000002|\n",
      "|      20100301|529839.6000000002|\n",
      "|      20100401|490967.0900000001|\n",
      "|      20100501|748098.0000000003|\n",
      "|      20100601|611528.2600000001|\n",
      "|      20100701|584327.2599999997|\n",
      "|      20100801|622679.6900000004|\n",
      "+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = results\\\n",
    ".select(\"OrderMonthYear\", \"sum\")\\\n",
    ".rdd\\\n",
    ".map(lambda r: LabeledPoint(r[1], [r[0]]))\\\n",
    ".toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|     features|            label|\n",
      "+-------------+-----------------+\n",
      "|[2.0090101E7]|734559.3599999996|\n",
      "|[2.0090201E7]|539887.7999999998|\n",
      "|[2.0090301E7]|559449.9600000002|\n",
      "|[2.0090401E7]|        614983.31|\n",
      "|[2.0090501E7]|637481.3899999998|\n",
      "|[2.0090601E7]|555516.2199999999|\n",
      "|[2.0090701E7]|        670807.13|\n",
      "|[2.0090801E7]|660353.6399999995|\n",
      "|[2.0090901E7]|        648992.75|\n",
      "|[2.0091001E7]|570492.5099999999|\n",
      "|[2.0091101E7]|566171.2399999998|\n",
      "|[2.0091201E7]|560232.9200000002|\n",
      "|[2.0100101E7]|577908.3700000001|\n",
      "|[2.0100201E7]|586651.7600000002|\n",
      "|[2.0100301E7]|529839.6000000002|\n",
      "|[2.0100401E7]|490967.0900000001|\n",
      "|[2.0100501E7]|748098.0000000003|\n",
      "|[2.0100601E7]|611528.2600000001|\n",
      "|[2.0100701E7]|584327.2599999997|\n",
      "|[2.0100801E7]|622679.6900000004|\n",
      "+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ba89caf5a42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.'"
     ]
    }
   ],
   "source": [
    "modelA = lr.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct<type:tinyint,size:int,indices:array<int>,values:array<double>> <br>\n",
    "struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
