{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "review_path = \"yelp_dataset/review.json\"\n",
    "business_path = 'yelp_dataset/business.json'\n",
    "\n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Son') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = ss.sparkContext\n",
    "\n",
    "reviewRDD = sc.textFile(review_path)\n",
    "\n",
    "review = reviewRDD.map(lambda line: json.loads(line)) \\\n",
    "    .map(lambda line: (line['user_id'], line['business_id'])) \\\n",
    "    .collect()\n",
    "\n",
    "review_list = list(zip(*review))\n",
    "\n",
    "businessRDD = sc.textFile(business_path)\n",
    "\n",
    "business = businessRDD.map(lambda line: json.loads(line)) \\\n",
    "    .map(lambda line: (line['business_id'], line['state'])) \\\n",
    "    .collect()\n",
    "\n",
    "business_list = list(zip(*business))\n",
    "\n",
    "review_dict = {\"user_id\": review_list[0], \"business_id\": review_list[1]}\n",
    "business_dict = {\"business_id\": business_list[0], \"state\": business_list[1]}\n",
    "\n",
    "review_df = pd.DataFrame.from_dict(review_dict)\n",
    "business_df = pd.DataFrame.from_dict(business_dict)\n",
    "\n",
    "print(review_df.head())\n",
    "print(business_df.head())\n",
    "\n",
    "all_df = pd.merge(left=review_df, right=business_df, how='left', left_on=[\"business_id\"], right_on=['business_id'])\n",
    "\n",
    "print(all_df.head())\n",
    "\n",
    "all_df_NV = all_df[all_df['state'] == 'NV']\n",
    "\n",
    "print(all_df_NV.head())\n",
    "\n",
    "del all_df_NV['state']\n",
    "\n",
    "all_df_NV.to_csv('task2_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(item_set, length):\n",
    "    '''\n",
    "    :param item_set: list[set()]\n",
    "    :param length: int\n",
    "    :return: list[set()]\n",
    "    '''\n",
    "    return_list = []\n",
    "    for item_1 in item_set:\n",
    "        for item_2 in item_set:\n",
    "            temp = item_1.union(item_2)\n",
    "            if temp not in return_list and len(temp) == length:\n",
    "                return_list.append(temp)\n",
    "    return return_list\n",
    "\n",
    "def frequent_items(items, data, support):\n",
    "    '''\n",
    "    :param items: list[set()]\n",
    "    :param data: list[list]\n",
    "    :param support: int\n",
    "    :return: list[set()]\n",
    "    '''\n",
    "    return_ = []\n",
    "    count = {}\n",
    "    for line in data:\n",
    "        for item in items:\n",
    "            if item.issubset(line[1]):\n",
    "                if tuple(item) not in count:\n",
    "                    count[tuple(item)] = 1\n",
    "                else:\n",
    "                    count[tuple(item)] += 1\n",
    "    for key, value in count.items():\n",
    "        if value >= support:\n",
    "            if set(key) not in return_:\n",
    "                return_.append(set(key))\n",
    "    return return_\n",
    "\n",
    "def makedic(data):\n",
    "    '''\n",
    "    :param data: iterator\n",
    "    :return: list[tuple]\n",
    "    '''\n",
    "    return_key = []\n",
    "    return_value = []\n",
    "    for line in data:\n",
    "        for item in line[1]:\n",
    "            if item not in return_key:\n",
    "                return_key.append(item)\n",
    "                return_value.append(1)\n",
    "            else:\n",
    "                index = return_key.index(item)\n",
    "                return_value[index] += 1\n",
    "    return_list = list(zip(return_key, return_value))\n",
    "    return return_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import Apriori as A\n",
    "import json\n",
    "import time\n",
    "\n",
    "sample_path = \"task2_data.csv\"\n",
    "\n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Son') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = ss.sparkContext\n",
    "\n",
    "start1 = time.time()\n",
    "smallRDD = sc.textFile(sample_path)\n",
    "header = smallRDD.first()\n",
    "\n",
    "small1RDD = smallRDD.filter(lambda row: row != header) \\\n",
    "    .map(lambda line: (line.split(',')[0], line.split(',')[1])) \\\n",
    "    .combineByKey(lambda line: [line],\n",
    "                  lambda exit, new: exit + [new],\n",
    "                  lambda exit1, exit2: exit1 + exit2)\n",
    "\n",
    "candidates = {}\n",
    "frequent = {}\n",
    "\n",
    "num_partitions = small1RDD.getNumPartitions()\n",
    "print(num_partitions)\n",
    "support = 4\n",
    "\n",
    "temp = small1RDD.mapPartitions(lambda data: A.makedic(data)).reduceByKey(lambda a, b: a + b).collect()\n",
    "temp_1 = {tup[0]: tup[1] for tup in temp}\n",
    "candidates[1] = list(temp_1.values())\n",
    "candidates[1] = [{item} for item in candidates[1]]\n",
    "frequent[1] = [{key} for key, value in temp_1.items() if value >= support]\n",
    "freq = sc.parallelize(frequent[1]).persist()\n",
    "\n",
    "k = 2\n",
    "\n",
    "while 1:\n",
    "    print(k)\n",
    "    candidate_temp = A.create_candidates(freq.collect(), k)\n",
    "    freq.unpersist()\n",
    "    freq = small1RDD.mapPartitions(lambda data: A.frequent_items(candidate_temp, data, support / num_partitions)) \\\n",
    "        .map(lambda x: (tuple(x), 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .filter(lambda a: a[1] >= num_partitions) \\\n",
    "        .map(lambda a: set(a[0])) \\\n",
    "        .cache()\n",
    "    fr = freq.collect()\n",
    "    if len(fr) == 0:\n",
    "        break\n",
    "    else:\n",
    "        candidates[k] = candidate_temp\n",
    "        frequent[k] = fr\n",
    "        k += 1\n",
    "\n",
    "for key, value in candidates.items():\n",
    "    candidates[key] = sorted([tuple(item) for item in value])\n",
    "# print(candidates)\n",
    "with open(\"candidates.json\", \"w\") as file:\n",
    "    json.dump(candidates, file, indent=1)\n",
    "for key, value in frequent.items():\n",
    "    frequent[key] = sorted([tuple(item) for item in value])\n",
    "# print(frequent)\n",
    "with open('frequent.json', 'w') as file:\n",
    "    json.dump(frequent, file, indent=1)\n",
    "\n",
    "end1 = time.time()\n",
    "print(end1 - start1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
